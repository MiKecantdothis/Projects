# -*- coding: utf-8 -*-
"""ML_ETE_project_tri2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vy-6Z1Vpz50VRglO8_6OBjhHN_UWU3fr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
nltk.download('stopwords')
nltk.download('wordnet')
STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from wordcloud import WordCloud
import re

data = pd.read_csv("/content/amazon_alexa.tsv", delimiter='\t')
data.head()

data.info()

data.dropna(inplace = True)

data['length'] = data['verified_reviews'].apply(len)
data.head()

print(data['rating'].value_counts())

plt.bar(data['rating'].value_counts().index, data['rating'].value_counts())
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Ratings')
plt.show()

data['feedback'] = data['rating'].apply(lambda x: '2' if x >= 4 else '0' if x <= 2 else '1')
data['feedback'] = data['feedback'].astype(int)
data.head()

data['feedback'].value_counts()

plt.pie(data['variation'].value_counts(), labels = data['variation'].value_counts().index, autopct = '%1.1f%%')

from itertools import groupby
data.groupby('variation')['feedback'].value_counts().plot.bar()
plt.title('Variation vs Feedback')
plt.xlabel('Variation and Feedback')
plt.ylabel('Count')
plt.show()

cv = CountVectorizer(stop_words = 'english')
words = cv.fit_transform(data['verified_reviews'])

review = " ".join(review for review in data.verified_reviews)

WC = WordCloud(width = 500, height = 500).generate(review)
plt.figure(figsize = (15, 10))
plt.imshow(WC)

from nltk.stem import PorterStemmer
corpus = []
stemmer = PorterStemmer()
for review in data['verified_reviews']:
  review = re.sub('[^a-zA-Z]', ' ', review)
  review = review.lower()
  review = review.split()
  review = [stemmer.stem(word) for word in review if word not in STOPWORDS]
  review = ' '.join(review)
  corpus.append(review)

cv = CountVectorizer(stop_words = 'english')
X = cv.fit_transform(corpus)
y = data['feedback']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean = False)
X_train_scl = sc.fit_transform(X_train)
X_test_scl = sc.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train_scl, y_train)
knn.score(X_test_scl, y_test)
y_pred = knn.predict(X_test_scl)

print(("test accuracy") , (knn.score(X_test_scl, y_test)))
print(("train accuracy") , (knn.score(X_train_scl, y_train)))

accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot = True)
plt.title('Confusion Matrix for KNN')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(X_train_scl, y_train)
y_pred2 = rf.predict(X_test_scl)

rf.score(X_test_scl, y_test)

cm2 = confusion_matrix(y_test, y_pred2)
sns.heatmap(cm2, annot = True)
plt.title('Confusion Matrix for Random Forest Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.model_selection import cross_val_score
accuries = cross_val_score(estimator = rf, X = X_train_scl, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuries.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuries.std()*100))

params = {
    'bootstrap': [True],
    'max_depth': [80,100],
    'min_samples_split' : [8,10],
    'n_estimators': [100,200]
}

import matplotlib.pyplot as plt

# Box plot for cross-validation scores
plt.figure(figsize=(8, 6))
plt.boxplot(accuries, vert=False, patch_artist=True, boxprops=dict(facecolor="lightblue"))
plt.title("Cross-Validation Accuracy Distribution")
plt.xlabel("Accuracy")
plt.yticks([1], ["RF"])
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()

# Histogram for cross-validation scores
plt.figure(figsize=(8, 6))
plt.hist(accuries, bins=10, color="skyblue", edgecolor="black", alpha=0.7)
plt.title("Cross-Validation Accuracy Distribution")
plt.xlabel("Accuracy")
plt.ylabel("Frequency")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

cv_object = GridSearchCV(estimator = rf, param_grid = params, cv = 3, n_jobs = -1, verbose = 2)
cv_object.fit(X_train_scl, y_train)

print("best parameters: ", cv_object.best_params_)
print("best score: ", cv_object.best_score_)

y_train.unique()

from sklearn.ensemble import AdaBoostClassifier
abc = AdaBoostClassifier(n_estimators = 100)
abc.fit(X_train_scl, y_train)
y_pred = abc.predict(X_test_scl)

feature_importances = abc.feature_importances_
plt.bar(range(len(feature_importances)), feature_importances)
plt.xlabel("Feature Index")
plt.ylabel("Importance")
plt.title("Feature Importances in AdaBoost")
plt.show()

#PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X_train_pca = pca.fit_transform(X_train_scl.toarray())
X_test_pca = pca.transform(X_test_scl.toarray())

import numpy as np
import matplotlib.pyplot as plt

# Train models on PCA-transformed data
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pca, y_train)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_pca, y_train)

abc = AdaBoostClassifier(n_estimators=100, random_state=42)
abc.fit(X_train_pca, y_train)

from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np

# Function to plot multiclass ROC curve
def plot_multiclass_roc_curve(model, X_test, y_test, title, n_classes):
    # Binarize the labels for multiclass ROC calculation
    y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))

    # Predict probabilities
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)
    else:
        raise ValueError(f"{model.__class__.__name__} does not support predict_proba.")

    # Plot ROC curve for each class
    plt.figure()
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
        auc = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])
        plt.plot(fpr, tpr, label=f"Class {i} (AUC = {auc:.2f})")

    # Plot random chance line
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend()
    plt.show()

# Number of classes in your target
n_classes = len(np.unique(y_test))

# Plot for each model
plot_multiclass_roc_curve(knn, X_test_pca, y_test, "KNN Multiclass ROC Curve", n_classes)
plot_multiclass_roc_curve(rf, X_test_pca, y_test, "Random Forest Multiclass ROC Curve", n_classes)
plot_multiclass_roc_curve(abc, X_test_pca, y_test, "AdaBoost Multiclass ROC Curve", n_classes)

